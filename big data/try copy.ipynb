{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.4\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 19:53:46 WARN Utils: Your hostname, Chirags-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.11 instead (on interface en0)\n",
      "24/12/26 19:53:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/26 19:53:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"CreateRDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"CSVExample\").getOrCreate()\n",
    "\n",
    "# Load CSV into a DataFrame\n",
    "file_path = \"2015-summary.csv\"  # Replace with the actual path\n",
    "flight_data_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# Display the DataFrame schema\n",
    "flight_data_df.printSchema()\n",
    "\n",
    "# Show the first few rows\n",
    "flight_data_df.show(5)\n",
    "type(flight_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file using DataSource API\n",
    "flight_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "\n",
    "# Display the DataFrame schema\n",
    "flight_data_df.printSchema()\n",
    "\n",
    "# Show the first few rows\n",
    "flight_data_df.show(5)\n",
    "print(type(flight_data_df))\n",
    "flight_data_rdd = flight_data_df.rdd\n",
    "print(type(flight_data_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count', 'United States,Romania,15', 'United States,Croatia,1', 'United States,Ireland,344', 'Egypt,United States,15']\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Read the file using SparkContext\n",
    "file_rdd = sc.textFile(file_path)\n",
    "\n",
    "# Display the first few lines\n",
    "print(file_rdd.take(5))\n",
    "print(type(file_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
      "0     United States             Romania     15\n",
      "1     United States             Croatia      1\n",
      "2     United States             Ireland    344\n",
      "3             Egypt       United States     15\n",
      "4     United States               India     62\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344), Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spth = '2015-summary.csv'\n",
    "pd_flightData2015 = pd.read_csv(spth,header = 0)\n",
    "print(type(pd_flightData2015))\n",
    "print(pd_flightData2015.head(5))\n",
    "pd_flightData2015 = spark.createDataFrame(pd_flightData2015).rdd\n",
    "print(type(pd_flightData2015))\n",
    "print(pd_flightData2015.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'The',\n",
       " 'Definitive',\n",
       " 'Guide',\n",
       " ':',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Processing',\n",
       " 'Made',\n",
       " 'Simple']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "myCollection\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "words.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myWords\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "['Spark', 'The', 'Definitive', 'Guide', ':']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### from a collection of text\n",
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "words.setName(\"myWords\")\n",
    "print(words.name())\n",
    "print(type(words))\n",
    "print(words.take(5))\n",
    "words.getNumPartitions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#from range of numbers\n",
    "myRange = spark.range(1000).toDF(\"number\").rdd.map(lambda row: row[0])\n",
    "print(myRange.take(5))\n",
    "print(type(myRange))\n",
    "print(myRange.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')#list\n",
    "    date = fields[0]\n",
    "    p_open = fields[1]\n",
    "    p_close = fields[5]\n",
    "    return (date, p_open, p_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'Open', 'Close'), ('2022-01-01', '100', '10000')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spth=\"RELIANCE.csv\"\n",
    "sdt=spark.sparkContext.textFile(spth)\n",
    "sdt=sdt.map(parseLine)\n",
    "sdt.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[(datetime.date(2022, 1, 1), 100, 10000), (datetime.date(2022, 1, 2), 200, 20000)]\n",
      "[(datetime.date(2022, 1, 1), 100, 10000), (datetime.date(2022, 1, 2), 200, 20000), (datetime.date(2022, 1, 3), 300, 30000), (datetime.date(2022, 1, 4), 400, 40000), (datetime.date(2022, 1, 5), 500, 50000)]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "spth=\"RELIANCE.csv\"\n",
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd \\\n",
    "    .map(lambda row: (row[0], row[1], row[5]))\n",
    "print(o_sdt.count())\n",
    "print(type(o_sdt))\n",
    "print(o_sdt.take(2))\n",
    "\n",
    "o_sdt=o_sdt.filter(lambda row: row[2] > row[1])\n",
    "print(o_sdt.take(5))\n",
    "print(type(o_sdt))\n",
    "print(o_sdt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HighClose(row):\n",
    "    ## Discuss\n",
    "    if row[2] > row[1]:\n",
    "        return(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[('2022-01-01', '100', '10000'), ('2022-01-02', '200', '20000'), ('2022-01-03', '300', '30000'), ('2022-01-04', '400', '40000'), ('2022-01-05', '500', '50000')]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5]))\n",
    "print(o_sdt.count())\n",
    "o_sdt=o_sdt.filter(lambda row: HighClose(row))\n",
    "print(o_sdt.take(5))\n",
    "print(type(o_sdt))\n",
    "print(o_sdt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[(datetime.date(2022, 1, 1), 100, 10000), (datetime.date(2022, 1, 2), 200, 20000), (datetime.date(2022, 1, 3), 300, 30000), (datetime.date(2022, 1, 4), 400, 40000), (datetime.date(2022, 1, 5), 500, 50000)]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5]))\n",
    "print(o_sdt.count())\n",
    "o_sdt=o_sdt.filter(lambda row: HighClose(row))\n",
    "print(o_sdt.take(5))\n",
    "print(type(o_sdt))\n",
    "print(o_sdt.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_to_mill(row):\n",
    "    return (row[0], row[1], row[2], round(row[3],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2022, 1, 1), 100, 10000, None), (datetime.date(2022, 1, 2), 200, 20000, None)]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "spth=\"RELIANCE.csv\"\n",
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5], row[7]))\n",
    "print(o_sdt.take(2))\n",
    "o_sdt=o_sdt.map(to_to_mill)\n",
    "print(type(o_sdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Func(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split(\" \")\n",
    "    return lines\n",
    "\n",
    "#sc.stop()\n",
    "#conf = SparkConf().setMaster(\"local\").setAppName(\"wordcount\")\n",
    "#sc = SparkContext.getOrCreate()\n",
    "\n",
    "spth=\"/Users/aakash/Downloads/spark-code/data/sherlock_holmes.txt\"\n",
    "input_file = sc.textFile(spth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
